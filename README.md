# AmazonPriceTracker
A web app to alert users of future price drops. Check it out at https://prizo.pythonanywhere.com/

This project is built using Django (the web framework), SQLite (the database), Scrapy (the web crawler), Bootstrap (front-end toolkit) and Python Anywhere (host).

## How it Works
When a user enters a URL on the website, a scrapy crawler is triggered to go check the URL and make sure that the product details are extractable (basically checks that it is a valid URL). If it is, this product is inserted into the DB and the user is prompted for his email address if they would like price updates to be emailed to them. Once per day, the server asynchronously launches a script to check the prices of all the products in the DB on amazon. It then stores all of these new prices in the DB and if any price has changed, it notifies the corresponding emails.

## Tour
The main page. Enter the product url you would like to start tracking
<img width="1440" alt="Main Page" src="https://user-images.githubusercontent.com/33245117/146040494-a94a676c-96b0-44f5-a754-658f480dc4b7.png">

The product page. Shows relevant price history and gives you an option to enter your email to get alerted of price drops.
<img width="1440" alt="Product Page" src="https://user-images.githubusercontent.com/33245117/146040822-10cd5357-5954-41bd-b47f-f1c57af91cd6.png">

The script runs once every 24 hours in the background, scraping through all the products in the database and alerting any relevant users of price drops

## Future Improvements
The inspiration for this project was to generate an income by sending affiliate links to users when alerting them of price drops. To do that, I will have to be approved as an amazon affiliate and replace and replace the links in the emails I send accordingly.

## Reproduce the code locally

To run this project, you will need to install all the appropriate dependencies

### Step 1
Create and activate a virtual environment. If not sure how to do so, refer to https://realpython.com/python-virtual-environments-a-primer/

### Step 2
Install all the dependencies. Navigate to the outer ProjectFolder with the "requirements.txt" file and execute the following command
```
pip install -r requirements.txt
```
### Step 3
Configure ProjectFolder/settings.py to send emails. The easiest way is via a google account. You will have to enable your google account for this via [this setting](https://myaccount.google.com/lesssecureapps). Then fill in the settings shown before

```
DEFAULT_FROM_EMAIL = '' #The email you will be sending from

EMAIL_BACKEND = 'django.core.mail.backends.smtp.EmailBackend'
EMAIL_HOST = 'smtp.gmail.com'
EMAIL_HOST_USER = '' #The email you will be sending from
EMAIL_HOST_PASSWORD = '' #Password for your email
EMAIL_PORT = 587
EMAIL_USE_TLS = True
```

### Step 4
Schedule the daily crawl. Using cron should be the easiest on unix systems. 
```
python manage.py crawl
```
This script will
1. Crawl all the products in the database daily and
2. Alert the appropriate users in case of any price drops

### Step 5
Launch the project
```
python manage.py runserver --noreload --nothreading
```

Now the project is fully functional. Visit the url generated by the above command to interact with the web app.

## Challenges

Since I was integrating multiple technologies together, it was really difficult to choose technologies and frameworks that would play nice with each other. Firstly, there were a lot of hacky solutions to integration problems. For example, to get the output of the spider back into my Django process, I had to store the output in text files and then read the output from there. Also, I kept running into the infamous ReactorNotRestartable issue in the Scrapy library. To fix this, I had to implement multiprocessing so that I could fork the process instead of restarting the Reactor.

Also, lots of unexpected complications constantly arose throughout development. For example, due to Scrapy's implementation that uses Twisted Reactors, I had to disable Django's quick reload and threading capabilities. And implementing the daily asynchronous script that checks for price updates proved to be a huge problem for me too. After spending days implementing Celery (a distributed task queue for Django) when it came time to host my project turns out it wasn't even supported by PythonAnywhere. Eventually I realised that there was in-built support for one async script in PythonAnywhere and that is what I eventually used.

Despite the amount of time I spent researching the different tools, technologies, frameworks and libraries, I kept running into integration errors and it made me realise how complex software development can be.

## Post - Mortem
Apparently something went wrong and the app now flags all links as invalid. I suspect that this has to do with Amazon.sg changing the names/ids of their html tags.

Also -> bad software design since
1. Repeated code, need to change html parsing code in multiple places
2. Write a design doc, a bit hard to navigate when coming back to it


